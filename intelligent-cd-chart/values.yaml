# Global configuration

global:
  namespace: intelligent-cd

inference:
  model: "my-model"
  url: "https://URL:PORT/v1"
  apiToken: ""
  tlsVerify: "true"

llamaStack:
  name: llama-stack
  # Optional: Override the default distribution name "rh-dev" with a custom image
  image: "quay.io/opendatahub/llama-stack:rhoai-v2.25-latest"
  route:
    enabled: true
  opentelemetry:
    enabled: true
    bucketNameSuffix: "tempo"
    otelServiceName: "llamastack"
    telemetrySinks: "console,otel_trace" # 'console, sqlite, otel_trace, otel_metric'
    otelExporterOtlpEndpoint: "http://otel-collector.otel.svc.cluster.local:4318"
  websearch:
    tavilyApiKey: ""

# S3/MinIO bucket configuration (shared across components)
s3Bucket:
  connection:
    secretName: minio-connection-secret
    username: "minio"
    password: "minio123"
    endpoint: "minio.minio.svc.cluster.local"
    port: '9000'
    region: "none"
    scheme: "http"

pipelines:
  enabled: true
  namespaceSuffix: "-pipelines"

RAG:
  enabled: true
  vector_db_id: "app-documentation"
  git_repo: "https://github.com/alpha-hack-program/intelligent-cd-iberia.git"
  git_branch: "main"
  git_docs_path: "intelligent-cd-docs"

gradioUI:
  name: gradio
  replicaCount: 1

  image:
    repository: "quay.io/alopezme/intelligent-cd-gradio"
    tag: "latest" # latest
    pullPolicy: Always

  exposure:
    type: "route" # Type of exposure: none, ingress, or route

  # Enable this to Apply resources from Gradio to the Cluster
  serviceAccount:
    create: true
    name: "gradio"
    rbac:
      clusterRole: "cluster-admin" # cluster-admin / view / edit

  config:
    argocd:
      base_url: "https://openshift-gitops-server.openshift-gitops:443"
      api_token: "<Token>"
    github:
      pat: "<Token>"
      toolsets: "pull_requests, repos, issues"
      readonly: "true"
    form_tab:
      # Global form configuration
      max_infer_iters: 50
      
      # Step 1: Generate Resources
      generate_resources:
        sampling_params:
          temperature: 0.1
          max_tokens: 300000
          max_new_tokens: 300000
          strategy:
            type: "greedy"
        tools:
          - "mcp::openshift"
          - name: "builtin::rag"
            args:
              vector_db_names:
                - "gitops-documentation"
              top_k: 5
      
      # Step 2: Generate Helm
      generate_helm:
        sampling_params:
          temperature: 0.1
          max_tokens: 300000
          max_new_tokens: 300000
          strategy:
            type: "greedy"
        tools: []
          # - "mcp::openshift"
          # - name: "builtin::rag"
          #   args:
          #     vector_db_names:
          #       - "gitops-documentation"
          #     top_k: 5
      
      # Step 3: Push GitHub
      push_github:
        sampling_params:
          temperature: 0.1
          max_tokens: 300000
          max_new_tokens: 300000
          strategy:
            type: "greedy"
        tools:
          - "mcp::github"
          - name: "builtin::rag"
            args:
              vector_db_names:
                - "gitops-documentation"
              top_k: 1
      
      # Step 4: Generate ArgoCD
      generate_argocd:
        sampling_params:
          temperature: 0.1
          max_tokens: 300000
          max_new_tokens: 300000
          strategy:
            type: "greedy"
        tools:
          # - "mcp::argocd"
          - "mcp::openshift"
          - name: "builtin::rag"
            args:
              vector_db_names:
                - "gitops-documentation"
              top_k: 1

    chat_tab:
      sampling_params:
        temperature: 0.1
        max_tokens: 300000
        max_new_tokens: 300000
        strategy:
          type: "greedy"
      tools:
        # - "mcp::servicenow"
        # - "mcp::argocd"
        - "mcp::openshift"
        - "mcp::github"
        - name: "builtin::rag"
          args:
            vector_db_names:
              - "app-documentation"
            top_k: 5
      max_infer_iters: 30

  env:
    LOG_LEVEL: "DEBUG" # DEBUG, INFO, WARNING, ERROR, CRITICAL
    GITHUB_GITOPS_REPO: ""

  # Resources
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "1500m"
      memory: "2Gi"

mcpServers:
  ocpMcpServer:
    name: ocp-mcp-server
    replicaCount: 1
    image:
      repository: "quay.io/alopezme/kubernetes-mcp-server"
      tag: "latest"

    mcpServer:
      port: 8080
      args:
        - "--sse-port"
        - "8080"
        - "--disable-destructive"
        - "--log-level"
        - "5"

    serviceAccount:
      create: true
      # Service account name (required if create=true, optional if create=false for existing SA)
      name: "ocp-mcp-server"

    rbac:
      clusterRole: "view" # cluster-admin / view / edit

  argocdMcpServer:
    name: argocd-mcp-server
    replicaCount: 1
    image:
      repository: "quay.io/alopezme/argocd-mcp"
      tag: "latest"
    mcpServer:
      # Port for the MCP server to listen on
      port: 3000
      # Add command to debug environment variables and start the server
      args:
        - "exec"
        - "argocd-mcp@latest"
        - "sse"
    serviceAccount:
      create: false
      # Service account name (required if create=true, optional if create=false for existing SA)
      name: default
      
  servicenowMcp:
    name: servicenow-mcp
    replicaCount: 1
    image:
      repository: "quay.io/alopezme/servicenow-mcp"
      tag: "latest"
    mcpServer:
      port: 8080
    env:
      SERVICENOW_INSTANCE_URL: "https://devInstance.service-now.com"
      SERVICENOW_USERNAME: "your-username"
      SERVICENOW_PASSWORD: "your-password"
      SERVICENOW_AUTH_TYPE: "basic"
      SERVICENOW_TOOL_PACKAGE: "service_desk"